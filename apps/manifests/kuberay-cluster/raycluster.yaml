apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: distributed-ai-cluster
  namespace: kuberay-system
  labels:
    ray.io/cluster: distributed-ai-cluster
spec:
  rayVersion: "2.50.1"
  enableInTreeAutoscaling: false
  autoscalerOptions:
    idleTimeoutSeconds: 600
    upscalingMode: Default
    resources:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"

  headGroupSpec:
    serviceType: ClusterIP
    rayStartParams:
      dashboard-host: "0.0.0.0"
      num-cpus: "55"
      memory: "236223201280" # 220Gi in bytes
      metrics-export-port: "8080"
      object-store-memory: "128849018880" # 120Gi
    template:
      metadata:
        labels:
          component: ray-head
          workload: distributed-ai
      spec:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: component
                        operator: In
                        values:
                          - ray-head
                          - ray-worker
                  topologyKey: kubernetes.io/hostname

        initContainers:
          - name: fix-permissions
            image: busybox
            command: ["sh", "-c"]
            args:
              - |
                mkdir -p /models/.cache/huggingface && chmod -R 777 /models
            volumeMounts:
              - mountPath: /models
                name: model-storage

          - name: install-transformers
            image: rayproject/ray:2.50.1-py310
            command: ["/bin/bash", "-c"]
            args:
              - |
                set -e
                echo "Installing dependencies for vllm compatibility..."
                mkdir -p /tmp/ray/pip-packages
                
                # Install with network resilience
                echo "[1/3] Installing OpenTelemetry packages (temporary, for vllm install)..."
                pip install --no-cache-dir --target=/tmp/ray/pip-packages \
                  --timeout 300 --retries 5 \
                  "opentelemetry-sdk>=1.26.0,<1.27.0" \
                  "opentelemetry-api>=1.26.0,<1.27.0" \
                  "opentelemetry-exporter-otlp>=1.26.0,<1.27.0" \
                  "opentelemetry-semantic-conventions-ai>=0.4.1,<0.5.0"
                
                echo "[2/3] Installing PyTorch, torchvision, and numpy..."
                pip install --no-cache-dir --target=/tmp/ray/pip-packages \
                  --timeout 300 --retries 5 \
                  "torch>=2.1.0" \
                  "torchvision>=0.16.0" \
                  "numpy<2.0"
                
                echo "[3/3] Installing vllm, transformers, and accelerate..."
                pip install --no-cache-dir --target=/tmp/ray/pip-packages \
                  --timeout 300 --retries 5 \
                  "vllm==0.7.1" \
                  "accelerate>=0.25.0" \
                  "transformers>=4.36.0"
                
                # CRITICAL: Remove packages that conflict with Ray's bundled versions
                # This allows Ray Dashboard to work with its bundled OpenTelemetry 1.19.0
                # while vllm uses the installed torch/transformers/accelerate
                echo "Cleaning up: removing conflicting packages..."
                rm -rf /tmp/ray/pip-packages/ray /tmp/ray/pip-packages/ray-* 2>/dev/null || true
                rm -rf /tmp/ray/pip-packages/opentelemetry* 2>/dev/null || true
                
                echo "Installation complete! Installed packages:"
                pip list --path=/tmp/ray/pip-packages | grep -E '(vllm|transformers|accelerate|torch)' || echo "Core packages installed"
            volumeMounts:
              - mountPath: /tmp/ray
                name: ray-temp

        containers:
          - name: ray-head
            image: rayproject/ray:2.50.1-py310
            imagePullPolicy: IfNotPresent
            resources:
              limits:
                cpu: "60"
                memory: "220Gi"
              requests:
                cpu: "55"
                memory: "220Gi"
            ports:
              - containerPort: 6379
                name: gcs
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
              - containerPort: 8080
                name: metrics
            livenessProbe:
              httpGet:
                path: /api/version
                port: 8265
              initialDelaySeconds: 30
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 5
            readinessProbe:
              httpGet:
                path: /api/version
                port: 8265
              initialDelaySeconds: 20
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 5
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]
            env:
              - name: PYTHONPATH
                value: "/tmp/ray/pip-packages"
              - name: RAY_GRAFANA_HOST
                value: "http://grafana.monitoring.svc:3000"
              - name: RAY_PROMETHEUS_HOST
                value: "http://prometheus.monitoring.svc:9090"
              - name: RAY_memory_monitor_refresh_ms
                value: "250"
              - name: RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE
                value: "1"
              - name: HF_HOME
                value: "/models/.cache/huggingface"
              - name: TRANSFORMERS_CACHE
                value: "/models/.cache/huggingface/transformers"
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
              - name: model-storage
                mountPath: /models
              - name: ray-temp
                mountPath: /tmp/ray

        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 150Gi
          - name: model-storage
            hostPath:
              path: /var/lib/models
              type: DirectoryOrCreate
          - name: ray-temp
            emptyDir:
              sizeLimit: 100Gi

  workerGroupSpecs:
    - groupName: ai-workers
      replicas: 2
      minReplicas: 2
      maxReplicas: 2
      rayStartParams:
        metrics-export-port: "8080"
        object-store-memory: "128849018880" # 120Gi
      template:
        metadata:
          labels:
            component: ray-worker
            workload: distributed-ai
            model-inference: "enabled"
        spec:
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: component
                        operator: In
                        values:
                          - ray-worker
                  topologyKey: kubernetes.io/hostname

          initContainers:
            - name: fix-permissions
              image: busybox
              command: ["sh", "-c"]
              args:
                - |
                  mkdir -p /models/.cache/huggingface && chmod -R 777 /models
              volumeMounts:
                - mountPath: /models
                  name: model-storage

            - name: install-transformers
              image: rayproject/ray:2.50.1-py310
              command: ["/bin/bash", "-c"]
              args:
                - |
                  set -e
                  echo "Installing dependencies for vllm compatibility..."
                  mkdir -p /tmp/ray/pip-packages
                  
                  # Install with network resilience
                  echo "[1/3] Installing OpenTelemetry packages (temporary, for vllm install)..."
                  pip install --no-cache-dir --target=/tmp/ray/pip-packages \
                    --timeout 300 --retries 5 \
                    "opentelemetry-sdk>=1.26.0,<1.27.0" \
                    "opentelemetry-api>=1.26.0,<1.27.0" \
                    "opentelemetry-exporter-otlp>=1.26.0,<1.27.0" \
                    "opentelemetry-semantic-conventions-ai>=0.4.1,<0.5.0"
                  
                  echo "[2/3] Installing PyTorch, torchvision, and numpy..."
                  pip install --no-cache-dir --target=/tmp/ray/pip-packages \
                    --timeout 300 --retries 5 \
                    "torch>=2.1.0" \
                    "torchvision>=0.16.0" \
                    "numpy<2.0"
                  
                  echo "[3/3] Installing vllm, transformers, and accelerate..."
                  pip install --no-cache-dir --target=/tmp/ray/pip-packages \
                    --timeout 300 --retries 5 \
                    "vllm==0.7.1" \
                    "accelerate>=0.25.0" \
                    "transformers>=4.36.0"
                  
                  # CRITICAL: Remove packages that conflict with Ray's bundled versions
                  # This allows Ray to work properly while vllm uses installed packages
                  echo "Cleaning up: removing conflicting packages..."
                  rm -rf /tmp/ray/pip-packages/ray /tmp/ray/pip-packages/ray-* 2>/dev/null || true
                  rm -rf /tmp/ray/pip-packages/opentelemetry* 2>/dev/null || true
                  
                  echo "Installation complete! Installed packages:"
                  pip list --path=/tmp/ray/pip-packages | grep -E '(vllm|transformers|accelerate|torch)' || echo "Core packages installed"
              volumeMounts:
                - mountPath: /tmp/ray
                  name: ray-temp

          containers:
            - name: ray-worker
              image: rayproject/ray:2.50.1-py310
              imagePullPolicy: IfNotPresent
              resources:
                limits:
                  cpu: "60"
                  memory: "220Gi"
                requests:
                  cpu: "55"
                  memory: "220Gi"
              ports:
                - containerPort: 8080
                  name: metrics
              livenessProbe:
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 30
                periodSeconds: 30
                timeoutSeconds: 10
                failureThreshold: 5
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              env:
                - name: PYTHONPATH
                  value: "/tmp/ray/pip-packages"
                - name: RAY_memory_monitor_refresh_ms
                  value: "250"
                - name: RAY_object_spilling_config
                  value: '{"type":"filesystem","params":{"directory_path":"/tmp/ray"}}'
                - name: RAY_max_direct_call_object_size
                  value: "100000000"
                - name: RAY_plasma_store_socket_name
                  value: "/tmp/ray/plasma_store"
                - name: RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE
                  value: "1"
                - name: HF_HOME
                  value: "/models/.cache/huggingface"
                - name: TRANSFORMERS_CACHE
                  value: "/models/.cache/huggingface/transformers"
                - name: OMP_NUM_THREADS
                  value: "60"
                - name: MKL_NUM_THREADS
                  value: "60"
              volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
                - name: ray-temp
                  mountPath: /tmp/ray
                - name: model-storage
                  mountPath: /models

          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 150Gi
            - name: ray-temp
              emptyDir:
                sizeLimit: 100Gi
            - name: model-storage
              hostPath:
                path: /var/lib/models
                type: DirectoryOrCreate