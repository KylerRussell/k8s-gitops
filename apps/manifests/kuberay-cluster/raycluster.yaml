apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: distributed-ai-cluster
  namespace: kuberay-system
  labels:
    ray.io/cluster: distributed-ai-cluster
spec:
  # Ray version - use specific version for stability
  rayVersion: "2.9.0"
  
  # Enable autoscaling for dynamic workloads
  enableInTreeAutoscaling: true
  
  # Autoscaler configuration
  autoscalerOptions:
    # Scale down idle workers after 10 minutes (longer for big models)
    idleTimeoutSeconds: 600
    upscalingMode: Default
    resources:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
  
  # Head node - minimal resources, control plane only
  headGroupSpec:
    serviceType: ClusterIP
    
    rayStartParams:
      dashboard-host: "0.0.0.0"
      # CRITICAL: Set to "0" to prevent task scheduling on head
      num-cpus: "0"
      # Head memory - enough for GCS and dashboard
      memory: "53687091200"  # 50GB in bytes
      metrics-export-port: "8080"
      # Object store MUST be less than /dev/shm size
      object-store-memory: "17179869184"  # 16GB (leaving 4GB headroom from 20GB shm)
    
    template:
      metadata:
        labels:
          component: ray-head
          workload: distributed-ai
      spec:
        # Node affinity to spread across servers
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: component
                        operator: In
                        values:
                          - ray-head
                          - ray-worker
                  topologyKey: kubernetes.io/hostname
        
        containers:
          - name: ray-head
            image: rayproject/ray:2.9.0
            imagePullPolicy: IfNotPresent
            
            # Head resources - control plane operations
            resources:
              limits:
                cpu: "8"         # Reduced from 16 to be more conservative
                memory: "32Gi"   # Reduced from 50Gi
              requests:
                cpu: "4"         # Requests lower than limits
                memory: "16Gi"
            
            ports:
              - containerPort: 6379
                name: gcs
                protocol: TCP
              - containerPort: 8265
                name: dashboard
                protocol: TCP
              - containerPort: 10001
                name: client
                protocol: TCP
              - containerPort: 8080
                name: metrics
                protocol: TCP
            
            # Health checks
            livenessProbe:
              httpGet:
                path: /api/version
                port: 8265
              initialDelaySeconds: 30
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 3
            
            readinessProbe:
              httpGet:
                path: /api/version
                port: 8265
              initialDelaySeconds: 15
              periodSeconds: 10
              timeoutSeconds: 5
            
            # Graceful shutdown
            lifecycle:
              preStop:
                exec:
                  command: ["/bin/sh", "-c", "ray stop"]
            
            # Environment for monitoring integration
            env:
              - name: RAY_GRAFANA_HOST
                value: "http://grafana.monitoring.svc:3000"
              - name: RAY_PROMETHEUS_HOST
                value: "http://prometheus.monitoring.svc:9090"
              # Memory management for large models
              - name: RAY_memory_monitor_refresh_ms
                value: "250"
              # Allow object store to use standard filesystem if needed
              - name: RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE
                value: "1"
            
            # Shared memory for object store
            volumeMounts:
              - name: dshm
                mountPath: /dev/shm
        
        volumes:
          - name: dshm
            emptyDir:
              medium: Memory
              sizeLimit: 20Gi
  
  # Worker group - MAXIMIZED for distributed AI workloads
  workerGroupSpecs:
    - groupName: ai-workers
      # Start with 2 workers, can scale 0-3
      replicas: 2
      minReplicas: 0
      maxReplicas: 3
      
      rayStartParams:
        # Workers use full resources - auto-detected from limits
        metrics-export-port: "8080"
        # Object store MUST be less than /dev/shm (100GB shm = 90GB object store)
        object-store-memory: "96636764160"  # 90GB (leaving 10GB headroom)
      
      template:
        metadata:
          labels:
            component: ray-worker
            workload: distributed-ai
            model-inference: "enabled"
        spec:
          # CRITICAL: Spread workers across different nodes
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: component
                        operator: In
                        values:
                          - ray-worker
                  topologyKey: kubernetes.io/hostname
          
          containers:
            - name: ray-worker
              image: rayproject/ray:2.9.0
              imagePullPolicy: IfNotPresent
              
              # Optimized resources for 357B model distribution
              # Conservative to ensure scheduling works
              resources:
                limits:
                  cpu: "56"        # Reduced slightly from 58
                  memory: "200Gi"  # Reduced from 220Gi to ensure scheduling
                requests:
                  cpu: "48"        # Requests lower than limits
                  memory: "180Gi"  # Requests lower than limits
              
              ports:
                - containerPort: 8080
                  name: metrics
                  protocol: TCP
              
              # Health monitoring
              livenessProbe:
                tcpSocket:
                  port: 8080
                initialDelaySeconds: 30
                periodSeconds: 30
                timeoutSeconds: 10
                failureThreshold: 5
              
              # Graceful shutdown with extended timeout for large models
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "ray stop"]
              
              # Environment optimizations for large AI models
              env:
                # Memory management
                - name: RAY_memory_monitor_refresh_ms
                  value: "250"
                # Object spilling for memory pressure
                - name: RAY_object_spilling_config
                  value: '{"type":"filesystem","params":{"directory_path":"/tmp/ray"}}'
                # Optimize for large objects
                - name: RAY_max_direct_call_object_size
                  value: "100000000"  # 100MB
                # Plasma store configuration
                - name: RAY_plasma_store_socket_name
                  value: "/tmp/ray/plasma_store"
                # Allow slow storage as fallback
                - name: RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE
                  value: "1"
              
              # Large shared memory for object store
              volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
                - name: ray-temp
                  mountPath: /tmp/ray
          
          volumes:
            - name: dshm
              emptyDir:
                medium: Memory
                # 100GB shared memory per worker for model weights
                sizeLimit: 100Gi
            - name: ray-temp
              emptyDir:
                sizeLimit: 50Gi  # Spillover storage
